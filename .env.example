# =============================================================================
# SWARM-MEMORY ENVIRONMENT CONFIGURATION
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# Load with:
#   source .env
#   # or
#   set -a; source .env; set +a

# =============================================================================
# LLM API KEYS
# =============================================================================

# OpenAI (for GPT models and embeddings API)
OPENAI_API_KEY=

# Anthropic (for Claude API - alternative to CLI)
ANTHROPIC_API_KEY=

# =============================================================================
# LLM PROVIDER SETTINGS
# =============================================================================

# Default LLM provider: claude, openai, ollama, hybrid
SWARM_LLM_PROVIDER=claude

# Override model for the chosen provider
# Examples: gpt-4o-mini, claude-3-haiku, llama3.2:3b
SWARM_LLM_MODEL=

# LLM tier for hybrid mode: fast, code, smart, auto
SWARM_LLM_TIER=auto

# Ollama host (for local models)
OLLAMA_HOST=http://localhost:11434

# =============================================================================
# MEMORY DATABASE
# =============================================================================

# Path to SQLite memory database (default: ./memory.db)
SWARM_MEMORY_DB=

# Alternative variable name
MEM_DB_PATH=

# =============================================================================
# EMBEDDINGS
# =============================================================================

# Embedding backend: local (sentence-transformers) or api (OpenAI)
EMBEDDING_BACKEND=local

# Local embedding model (for sentence-transformers)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# =============================================================================
# API SERVER (for cross-machine access)
# =============================================================================

# Memory server host and port
MEM_SERVER_HOST=0.0.0.0
MEM_SERVER_PORT=8765

# =============================================================================
# DAEMON SETTINGS
# =============================================================================

# Default repository root for daemon actions
SWARM_REPO_ROOT=

# Max iterations before daemon stops
SWARM_MAX_ITERATIONS=100

# Enable/disable governor pre-flight checks (true/false)
SWARM_USE_GOVERNOR=true

# =============================================================================
# IDEA CONSOLIDATION
# =============================================================================

# Minimum ideas before consolidation triggers
SWARM_CONSOLIDATE_MIN_IDEAS=5

# Maximum ideas to process in one consolidation
SWARM_CONSOLIDATE_MAX_IDEAS=30

# Consolidation interval (every N daemon iterations)
SWARM_CONSOLIDATE_INTERVAL=10

# =============================================================================
# LOGGING
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
SWARM_LOG_LEVEL=INFO

# Log full LLM responses (true/false)
SWARM_LOG_FULL_RESPONSE=false
