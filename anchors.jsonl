# Legend: [type, topic, text, choice, rationale, ts, session, source]
# Types: d=decision, q=question, a=action, f=fact, n=note
["d","memory","Use mini-JSON over symbolic DSL for anchor storage","mini-json","Models have seen tons of JSON; symbolic DSL brittle with cheap models","2025-11-29T03:00:00Z","memory_planning","claude-chat"]
["d","hierarchy","Limit agent hierarchy to 2-3 hops maximum","shallow","Each hop is lossy compression; deeper = more signal loss","2025-11-29T03:15:00Z","memory_planning","claude-chat"]
["d","temporal","Store timestamps separately, apply decay at query time","query-time-decay","Keeps embeddings reusable; tau tunable per use case","2025-11-29T03:30:00Z","memory_research","codex-agent"]
["d","chunks","Use 256-384 token chunks with 20-30% overlap","320-64","Consensus across SynapticRAG, TempRALM, BEAM research","2025-11-29T03:45:00Z","memory_research","codex-agent"]
["f","compression","Summarization achieves 3-10x safe compression, 15-30x lossy","","From Letta/MemGPT research","2025-11-29T04:00:00Z","memory_research","codex-agent"]
["f","format","Fixed-order arrays save 30-40% tokens vs keyed JSON","","Tested: [\"d\",\"topic\",\"text\"] vs {\"t\":\"d\",\"topic\":\"...\"}","2025-11-29T04:15:00Z","memory_research","codex-agent"]
["q","embedding","Which embedding model for local use?","","Candidates: e5-large-v2, nomic-embed-text-v1.5","2025-11-29T04:30:00Z","memory_planning","open"]
["a","research","Launch 4 Codex agents for memory system research","completed","Temporal, Anchor, Production, Compression tracks","2025-11-29T05:00:00Z","memory_research","claude-code"]
["n","insight","Letta uses 3 tiers: core (in-context), recall (vector), archive (compressed)","","Maps well to our anchors + chunks + summaries structure","2025-11-29T05:15:00Z","memory_research","synthesis"]
["d","temporal","Encode time via sinusoidal embeddings + exponential decay at retrieval","sinusoidal+decay","Separates time from text embeddings, allows tunable decay parameter","","research_session","log-extract"]
["d","hierarchy","Use 3-level hierarchy: 256-384 token leaves, 1k session summaries, 2k long-term summaries","3-level","Balances granularity with compression, matches BEAM/LIGHT patterns","","research_session","log-extract"]
["d","chunking","320 token chunks with 64 token overlap","320/64","Standard across TempRALM and recommendations, prevents context loss","","research_session","log-extract"]
["d","embeddings","Use E5-large-v2 or nomic-embed-text locally, text-embedding-3-large for API","E5/nomic/OpenAI","High-quality sentence embeddings with good OSS support","","research_session","log-extract"]
["d","storage","SQLite schema with id, parent_id, bucket, timestamp, embedding, text columns","SQLite","Simple, bash-friendly, supports hierarchy and temporal metadata","","research_session","log-extract"]
["d","retrieval","Combine vector similarity with time decay: score = sim + β·exp(-Δt/τ)","hybrid-scoring","Balances semantic relevance with recency, tunable via β and τ","","research_session","log-extract"]
["a","compression","Implement SeCom-style KMeans clustering for old chunks","","Reduces memory footprint for aged data","","research_session","log-extract"]
["f","decay-params","Typical decay tau = 7 days, beta = 0.3-0.4","","Standard parameters from TempRALM research","","research_session","log-extract"]
["f","agent-roles","Four roles: Ingestor, Summarizer, Retriever, Compressor","","Division of labor for bash pipeline","","research_session","log-extract"]
["n","verification","Unit-test decay scoring and dry-run parent/child retrieval","","Critical for validating temporal and hierarchical correctness","","research_session","log-extract"]
["d","embeddings","Use E5-large-v2 for local embeddings","E5-large-v2","Best balance of quality and speed for batch processing","2025-11-29T14:00:00Z","test_session","test"]
["a","memory","Build mem-search helper with filtering support","","","2025-11-29T14:00:00Z","test_session","test"]
["q","architecture","Is deeper hierarchy worth the complexity?","","","2025-11-29T14:00:00Z","test_session","test"]
["f","infrastructure","System runs Ubuntu 22.04 with 32GB RAM and RTX 4090","","","2025-11-29T14:00:00Z","test_session","test"]
["a","retrieval","Implement temporal decay in retrieval scoring with configurable tau","","","2025-11-29T14:00:00Z","test_session","test"]
["a","test","Test action for status update verification","done","Marked done for Test 4","2025-11-29T19:00:00Z","test4","test"]
["a","test","Test action for status update verification","done","Completed for test4","2025-11-29T19:05:00Z","test4","test"]
["d","embeddings","Use E5-large-v2 locally for embeddings","E5-large-v2","Local deployment choice","","test_session","log-extract"]
["a","memory","Implement mem-search helper","","","","test_session","log-extract"]
["q","architecture","Is deeper hierarchy worth the complexity?","","","","test_session","log-extract"]
["n","test","Integration test entry added by Claude Code","","","2025-11-30T19:00:00Z","integration_test","claude-code"]
["n", null, "Test note", null, null, "2025-11-30T20:19:27Z", null, null]\n["n", "testing", "Test note with metadata", null, null, "2025-11-30T20:19:37Z", null, null]\n["d", "auth", "Use JWT", "JWT tokens", "Industry standard", "2025-11-30T20:19:42Z", null, null]\n["f", null, "Found a bug", null, null, "2025-11-30T20:19:46Z", null, null]\n["a", "testing", "Test action", null, null, "2025-11-30T20:20:13Z", "test_session", "manual_test"]
["d", "architecture", "Use microservices", "microservices", "Better scalability", "2025-11-30T20:20:45Z", "design_review", "meeting_notes"]
["q", "database", "Should we use PostgreSQL or MySQL?", null, null, "2025-11-30T20:21:11Z", "planning", "brainstorm"]
["n", null, "Verification test", null, null, "2025-11-30T20:21:44Z", null, null]
["n", null, "assessment note", null, null, "2025-11-30T20:39:27Z", null, null]
["f", null, "role-based fact", null, null, "2025-11-30T20:51:29Z", null, null]
["n", null, "Fix test", null, null, "2025-11-30T20:55:24Z", null, null]
["n", null, "dup-check", null, null, "2025-11-30T21:55:39Z", null, null]
["d", "embeddings", "Use OpenAI API only for embeddings, no local ML deps", "api-only", "User has no GPU, ChatGPT Pro != API access", "2025-11-30T23:58:20Z", "phase4-5", null]
["d", "retrieval", "Hybrid scoring formula: score = \u03b1\u00b7sim + \u03b2\u00b7exp(-\u0394t/\u03c4)", "\u03b1=1.0, \u03b2=0.3, \u03c4=7", "Balances relevance with recency", "2025-11-30T23:58:25Z", "phase4-5", null]
["d", "workflow", "Claude actively uses memory during conversations - queries context, records decisions", "active-memory", null, "2025-11-30T23:59:23Z", "phase4-5", null]
["a", "git", "Pushed Phase 4-5 to github.com/badnewsgoonies-dot/swarm-memory", null, null, "2025-12-01T00:00:36Z", "phase4-5", null]
["f", "memory", "Cross-chat memory sharing verified working - new chat successfully queried decisions from this session", null, null, "2025-12-01T00:31:11Z", "phase4-5", null]
["f", "memory", "Chats default to reading anchors.jsonl directly instead of using CLI queries - need stronger CLAUDE.md instructions", null, null, "2025-12-01T00:35:47Z", "phase4-5", null]
["f", "memory", "CLAUDE.md instructions successfully guide chats to use CLI queries instead of reading raw JSONL", null, null, "2025-12-01T00:43:32Z", "phase4-5", null]
["d", "embeddings", "Use local embeddings with all-MiniLM-L6-v2, no API key required", "local-model", "Works on CPU, free, 384 dimensions", "2025-12-01T02:05:52Z", "phase4-5", null]
["d", "embeddings", "Local embeddings work without API key using sentence-transformers and all-MiniLM-L6-v2", "local-backend", "Installed on older PC, portable to 1060 PC", "2025-12-01T02:41:31Z", "memory-setup", null]
["f", "setup", "Venv with sentence-transformers needed for embeddings: python3 -m venv .venv && . .venv/bin/activate && pip install sentence-transformers", null, null, "2025-12-01T02:41:31Z", "memory-setup", null]
["f", "sync", "memory.db is gitignored - to sync across machines: git pull, then ./mem-db.sh init && ./mem-db.sh sync", null, null, "2025-12-01T02:41:31Z", "memory-setup", null]
