# Legend: [type, topic, text, choice, rationale, ts, session, source]
# Types: d=decision, q=question, a=action, f=fact, n=note
["d","memory","Use mini-JSON over symbolic DSL for anchor storage","mini-json","Models have seen tons of JSON; symbolic DSL brittle with cheap models","2025-11-29T03:00:00Z","memory_planning","claude-chat"]
["d","hierarchy","Limit agent hierarchy to 2-3 hops maximum","shallow","Each hop is lossy compression; deeper = more signal loss","2025-11-29T03:15:00Z","memory_planning","claude-chat"]
["d","temporal","Store timestamps separately, apply decay at query time","query-time-decay","Keeps embeddings reusable; tau tunable per use case","2025-11-29T03:30:00Z","memory_research","codex-agent"]
["d","chunks","Use 256-384 token chunks with 20-30% overlap","320-64","Consensus across SynapticRAG, TempRALM, BEAM research","2025-11-29T03:45:00Z","memory_research","codex-agent"]
["f","compression","Summarization achieves 3-10x safe compression, 15-30x lossy","","From Letta/MemGPT research","2025-11-29T04:00:00Z","memory_research","codex-agent"]
["f","format","Fixed-order arrays save 30-40% tokens vs keyed JSON","","Tested: [\"d\",\"topic\",\"text\"] vs {\"t\":\"d\",\"topic\":\"...\"}","2025-11-29T04:15:00Z","memory_research","codex-agent"]
["q","embedding","Which embedding model for local use?","","Candidates: e5-large-v2, nomic-embed-text-v1.5","2025-11-29T04:30:00Z","memory_planning","open"]
["a","research","Launch 4 Codex agents for memory system research","completed","Temporal, Anchor, Production, Compression tracks","2025-11-29T05:00:00Z","memory_research","claude-code"]
["n","insight","Letta uses 3 tiers: core (in-context), recall (vector), archive (compressed)","","Maps well to our anchors + chunks + summaries structure","2025-11-29T05:15:00Z","memory_research","synthesis"]
["d","temporal","Encode time via sinusoidal embeddings + exponential decay at retrieval","sinusoidal+decay","Separates time from text embeddings, allows tunable decay parameter","","research_session","log-extract"]
["d","hierarchy","Use 3-level hierarchy: 256-384 token leaves, 1k session summaries, 2k long-term summaries","3-level","Balances granularity with compression, matches BEAM/LIGHT patterns","","research_session","log-extract"]
["d","chunking","320 token chunks with 64 token overlap","320/64","Standard across TempRALM and recommendations, prevents context loss","","research_session","log-extract"]
["d","embeddings","Use E5-large-v2 or nomic-embed-text locally, text-embedding-3-large for API","E5/nomic/OpenAI","High-quality sentence embeddings with good OSS support","","research_session","log-extract"]
["d","storage","SQLite schema with id, parent_id, bucket, timestamp, embedding, text columns","SQLite","Simple, bash-friendly, supports hierarchy and temporal metadata","","research_session","log-extract"]
["d","retrieval","Combine vector similarity with time decay: score = sim + β·exp(-Δt/τ)","hybrid-scoring","Balances semantic relevance with recency, tunable via β and τ","","research_session","log-extract"]
["a","compression","Implement SeCom-style KMeans clustering for old chunks","","Reduces memory footprint for aged data","","research_session","log-extract"]
["f","decay-params","Typical decay tau = 7 days, beta = 0.3-0.4","","Standard parameters from TempRALM research","","research_session","log-extract"]
["f","agent-roles","Four roles: Ingestor, Summarizer, Retriever, Compressor","","Division of labor for bash pipeline","","research_session","log-extract"]
["n","verification","Unit-test decay scoring and dry-run parent/child retrieval","","Critical for validating temporal and hierarchical correctness","","research_session","log-extract"]
["d","embeddings","Use E5-large-v2 for local embeddings","E5-large-v2","Best balance of quality and speed for batch processing","2025-11-29T14:00:00Z","test_session","test"]
["a","memory","Build mem-search helper with filtering support","","","2025-11-29T14:00:00Z","test_session","test"]
["q","architecture","Is deeper hierarchy worth the complexity?","","","2025-11-29T14:00:00Z","test_session","test"]
["f","infrastructure","System runs Ubuntu 22.04 with 32GB RAM and RTX 4090","","","2025-11-29T14:00:00Z","test_session","test"]
["a","retrieval","Implement temporal decay in retrieval scoring with configurable tau","","","2025-11-29T14:00:00Z","test_session","test"]
["a","test","Test action for status update verification","done","Marked done for Test 4","2025-11-29T19:00:00Z","test4","test"]
["a","test","Test action for status update verification","done","Completed for test4","2025-11-29T19:05:00Z","test4","test"]
["d","embeddings","Use E5-large-v2 locally for embeddings","E5-large-v2","Local deployment choice","","test_session","log-extract"]
["a","memory","Implement mem-search helper","","","","test_session","log-extract"]
["q","architecture","Is deeper hierarchy worth the complexity?","","","","test_session","log-extract"]
["n","test","Integration test entry added by Claude Code","","","2025-11-30T19:00:00Z","integration_test","claude-code"]
["n", null, "Test note", null, null, "2025-11-30T20:19:27Z", null, null]\n["n", "testing", "Test note with metadata", null, null, "2025-11-30T20:19:37Z", null, null]\n["d", "auth", "Use JWT", "JWT tokens", "Industry standard", "2025-11-30T20:19:42Z", null, null]\n["f", null, "Found a bug", null, null, "2025-11-30T20:19:46Z", null, null]\n["a", "testing", "Test action", null, null, "2025-11-30T20:20:13Z", "test_session", "manual_test"]
["d", "architecture", "Use microservices", "microservices", "Better scalability", "2025-11-30T20:20:45Z", "design_review", "meeting_notes"]
["q", "database", "Should we use PostgreSQL or MySQL?", null, null, "2025-11-30T20:21:11Z", "planning", "brainstorm"]
["n", null, "Verification test", null, null, "2025-11-30T20:21:44Z", null, null]
["n", null, "assessment note", null, null, "2025-11-30T20:39:27Z", null, null]
["f", null, "role-based fact", null, null, "2025-11-30T20:51:29Z", null, null]
["n", null, "Fix test", null, null, "2025-11-30T20:55:24Z", null, null]
["n", null, "dup-check", null, null, "2025-11-30T21:55:39Z", null, null]
["d", "embeddings", "Use OpenAI API only for embeddings, no local ML deps", "api-only", "User has no GPU, ChatGPT Pro != API access", "2025-11-30T23:58:20Z", "phase4-5", null]
["d", "retrieval", "Hybrid scoring formula: score = \u03b1\u00b7sim + \u03b2\u00b7exp(-\u0394t/\u03c4)", "\u03b1=1.0, \u03b2=0.3, \u03c4=7", "Balances relevance with recency", "2025-11-30T23:58:25Z", "phase4-5", null]
["d", "workflow", "Claude actively uses memory during conversations - queries context, records decisions", "active-memory", null, "2025-11-30T23:59:23Z", "phase4-5", null]
["a", "git", "Pushed Phase 4-5 to github.com/badnewsgoonies-dot/swarm-memory", null, null, "2025-12-01T00:00:36Z", "phase4-5", null]
["f", "memory", "Cross-chat memory sharing verified working - new chat successfully queried decisions from this session", null, null, "2025-12-01T00:31:11Z", "phase4-5", null]
["f", "memory", "Chats default to reading anchors.jsonl directly instead of using CLI queries - need stronger CLAUDE.md instructions", null, null, "2025-12-01T00:35:47Z", "phase4-5", null]
["f", "memory", "CLAUDE.md instructions successfully guide chats to use CLI queries instead of reading raw JSONL", null, null, "2025-12-01T00:43:32Z", "phase4-5", null]
["d", "embeddings", "Use local embeddings with all-MiniLM-L6-v2, no API key required", "local-model", "Works on CPU, free, 384 dimensions", "2025-12-01T02:05:52Z", "phase4-5", null]
["d", "embeddings", "Local embeddings work without API key using sentence-transformers and all-MiniLM-L6-v2", "local-backend", "Installed on older PC, portable to 1060 PC", "2025-12-01T02:41:31Z", "memory-setup", null]
["f", "setup", "Venv with sentence-transformers needed for embeddings: python3 -m venv .venv && . .venv/bin/activate && pip install sentence-transformers", null, null, "2025-12-01T02:41:31Z", "memory-setup", null]
["f", "sync", "memory.db is gitignored - to sync across machines: git pull, then ./mem-db.sh init && ./mem-db.sh sync", null, null, "2025-12-01T02:41:31Z", "memory-setup", null]
["f", "memory", "CLAUDE.md must explicitly say 'query memory FIRST before exploring code' - chats default to file exploration otherwise", null, null, "2025-12-01T03:42:09Z", "memory-setup", null]
["d", "architecture", "Use knowledge graph (Zep-style) for memory, not just flat anchors", "knowledge-graph", "Better handling of conflicting/evolving facts via bi-temporal tracking and entity relationships", "2025-12-01T04:23:24Z", "planning-session", "claude-opus"]
["d", "architecture", "Use 2-layer sub-agent hierarchy: Head \u2192 Worker \u2192 Archive", "2-layers", "Each hop adds 1-3s latency; 2 layers balances depth vs speed", "2025-12-01T04:23:25Z", "planning-session", "claude-opus"]
["d", "scale", "Target 10K+ anchors for comprehensive knowledge base", "10k-plus", "User wants long-term multi-project memory, not just single session", "2025-12-01T04:23:25Z", "planning-session", "claude-opus"]
["d", "priority", "Prioritize retrieval accuracy over cost/latency", "accuracy-first", "User willing to sacrifice speed for getting the right memories", "2025-12-01T04:23:26Z", "planning-session", "claude-opus"]
["f", "research", "Zep achieves 94.8% accuracy on Deep Memory Retrieval benchmark vs MemGPT 93.4%", null, null, "2025-12-01T04:23:43Z", "planning-session", "claude-opus"]
["f", "research", "H-MEM uses 4-layer hierarchy with index pointers: Domain \u2192 Category \u2192 Trace \u2192 Episode", null, null, "2025-12-01T04:23:43Z", "planning-session", "claude-opus"]
["f", "research", "Mem0 uses extract/update pipeline: LLM extracts candidates, compares to vector DB, chooses add/merge/update/skip", null, null, "2025-12-01T04:23:44Z", "planning-session", "claude-opus"]
["f", "research", "Zep uses bi-temporal modeling: event_time (when fact occurred) + ingestion_time (when recorded) for conflict detection", null, null, "2025-12-01T04:23:44Z", "planning-session", "claude-opus"]
["f", "pitfalls", "Common memory pitfalls: bloat (solved by merge/dedupe), conflicts (bi-temporal tracking), lost-in-middle (hierarchical routing), models ignoring context (explicit formatting)", null, null, "2025-12-01T04:23:45Z", "planning-session", "claude-opus"]
["n", "novelty", "Novel elements: 1) Multi-model CLI orchestration (Claude/Codex/Copilot via bash), 2) Explicit head/sub-agent separation, 3) Query-time decay (more flexible than static), 4) Compact array format (30-40% token savings)", null, null, "2025-12-01T04:23:54Z", "planning-session", "claude-opus"]
["q", "knowledge-graph", "What schema for knowledge graph entities and relationships? Options: Neo4j, SQLite with edges table, or extend chunks table", null, null, "2025-12-01T04:23:55Z", "planning-session", "claude-opus"]
["d", "knowledge-graph", "Use SQLite with separate entities and edges tables for knowledge graph", "sqlite-entities-edges", "Add entities table for extracted concepts, edges table for relationships. Keep chunks as source of truth.", "2025-12-01T04:27:39Z", "planning-session", "user"]
["d", "architecture", "Hierarchical sub-agent memory retrieval: Head \u2192 Mid-tier \u2192 Leaf agents. Each tier cheaper/smaller. Data flows up as compressed summaries, sub-agents only see slices not full history", "hierarchical-retrieval", null, "2025-12-01T04:36:09Z", "early-planning", null]
["d", "memory-format", "Memory stored as compartmentalized blocks, not single summary. Each block: raw text, metadata (type/topic/time/origin), embeddings, AI-facing structure", "structured-blocks", null, "2025-12-01T04:36:10Z", "early-planning", null]
["d", "glyph-language", "AI-native compressed glyph language for memory. DSL format like [M:DESIGN][proj:X][t:timestamp] DEC: key=value WHY: reason. Super structured, short tokens, parseable. Track G: (grounded) vs I: (inferred)", "ai-glyphs", null, "2025-12-01T04:36:11Z", "early-planning", null]
["d", "context-budget", "Reserve ~half context window for memory refresh. Triggers: context usage >60%, novelty detected, user signals importance. Multi-pass refresh through tiers", "context-budgeting", null, "2025-12-01T04:36:41Z", "early-planning", null]
["d", "temporal", "Time as first-class axis in memory. Fields: timestamp, time_bucket (hour/day/week), session_id, seq_in_session. Enables 'last hour/day/week' queries", "time-indexed", null, "2025-12-01T04:36:42Z", "early-planning", null]
["f", "memory-risks", "Memory system risks: 1) Error stacking through multi-step summarization 2) Stale memory bias 3) AI fills gaps (hallucination) 4) Need forgetting/deprecation mechanism 5) User control over what persists", null, null, "2025-12-01T04:37:01Z", "early-planning", null]
["f", "memory-mitigation", "Mitigations: Keep raw refs in summaries (linked_ids), drill-down capability, multi-pass summarization (facts\u2192caveats\u2192combine), mark grounded vs inferred, recency weighting, superseding mechanism", null, null, "2025-12-01T04:37:02Z", "early-planning", null]
["d", "agent-roles", "Agent specialization: Top orchestrator (speaks normal + glyph), Mid-tier (read/write glyph memory, convert raw\u2192blocks), Leaf agents (handle full raw text/code, produce compact glyphs). Sub-agents dont chat, they write glyph-coded updates", "specialized-tiers", null, "2025-12-01T04:37:02Z", "early-planning", null]
["f", "reconstruction", "AI can reconstruct full picture from tiny snippets using world knowledge + priors. Store surgical extractions, expand when needed. Risk: expanded picture may be partly imagined. Mark model_inferred fields separately", null, null, "2025-12-01T04:37:03Z", "early-planning", null]
["d", "stack", "Current stack: Claude orchestrating Codex agents, ranging from 5.1 high thinking down to minis. Hierarchical swarm with job_pool, progress tracking already built", "claude-codex", null, "2025-12-01T04:37:04Z", "early-planning", null]
["d", "memory-schema", "Memory block schema: id, kind (conversation/decision/doc/code/bug/plan), project, created_at, updated_at, summary, body_ref, tags, embedding vector. Bonus: memory-of-memories index for important decisions", "block-schema", null, "2025-12-01T04:37:16Z", "early-planning", null]
["f", "glyph-example", "Glyph format example: [M:PREF][user:U1][t:2025-11-30] P: lang=TS strict=true P: style=verbose SRC: convo:2025-11-30#msg23-37 STATUS: ACTIVE. Add v: for versioning, G: for grounded, I: for inferred", null, null, "2025-12-01T04:37:16Z", "early-planning", null]
["f", "memory-validation", "A/B test proved memory system value: With memory=3 CLI queries, seconds. Without memory=read 5+ files, 1000+ lines, 10x longer for same answer", null, null, "2025-12-01T04:57:37Z", "validation-session", null]
["f", "cross-chat", "Fresh chat with /clear successfully used memory to answer questions it had no prior context for. Proves knowledge transfer works.", null, null, "2025-12-01T04:57:37Z", "validation-session", null]
["d", "claude-md", "CLAUDE.md must explicitly forbid: reading anchors.jsonl directly, launching Explore agents, reading source files for stored knowledge. Chats default to exploration without strong instructions.", "explicit-forbid", null, "2025-12-01T04:57:38Z", "validation-session", null]
["d", "embeddings", "Local embeddings with all-MiniLM-L6-v2 (384 dims) work without API key. sentence-transformers in .venv. Portable - copy memory.db anywhere.", "local-minilm", null, "2025-12-01T04:57:38Z", "validation-session", null]
["f", "workflow", "Recommended workflow: 1) Query memory first 2) Only explore code if memory lacks answer 3) Write key decisions/facts as you go 4) Embed new entries periodically", null, null, "2025-12-01T04:57:38Z", "validation-session", null]
["f", "setup", "Cross-machine sync: git pull gets anchors.jsonl, then ./mem-db.sh init && sync. memory.db is gitignored. Embeddings need venv setup on each machine.", null, null, "2025-12-01T04:57:39Z", "validation-session", null]
["d", "retrieval", "Semantic search uses hybrid scoring: score = \u03b1\u00b7sim + \u03b2\u00b7exp(-\u0394t/\u03c4). Defaults: \u03b1=1.0, \u03b2=0.3, \u03c4=7 days. Balances relevance with recency.", "hybrid-formula", null, "2025-12-01T04:57:54Z", "validation-session", null]
["f", "phases", "Phase 1-2: SQLite foundation. Phase 3: Multi-chat scoping. Phase 4: Embeddings pipeline. Phase 5: Semantic search. All complete and working.", null, null, "2025-12-01T04:57:54Z", "validation-session", null]
["q", "next-steps", "Potential next: auto-embed on write, knowledge graph (entities+edges tables), hooks for auto-inject, GPU acceleration on 1060", null, null, "2025-12-01T04:57:54Z", "validation-session", null]
["f", "testing", "Testing pattern: ask fresh chat a question, compare with vs without memory. Memory-enabled chat found answer in seconds, non-memory chat took 10x longer reading source files.", null, null, "2025-12-01T04:57:55Z", "validation-session", null]
["f", "repo", "GitHub repo: github.com/badnewsgoonies-dot/swarm-memory. Contains all memory system code, CLAUDE.md auto-instructions, anchors.jsonl data.", null, null, "2025-12-01T04:57:55Z", "validation-session", null]
