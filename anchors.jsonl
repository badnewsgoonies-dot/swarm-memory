# Legend: [type, topic, text, choice, rationale, ts, session, source]
# Types: d=decision, q=question, a=action, f=fact, n=note
["d","memory","Use mini-JSON over symbolic DSL for anchor storage","mini-json","Models have seen tons of JSON; symbolic DSL brittle with cheap models","2025-11-29T03:00:00Z","memory_planning","claude-chat"]
["d","hierarchy","Limit agent hierarchy to 2-3 hops maximum","shallow","Each hop is lossy compression; deeper = more signal loss","2025-11-29T03:15:00Z","memory_planning","claude-chat"]
["d","temporal","Store timestamps separately, apply decay at query time","query-time-decay","Keeps embeddings reusable; tau tunable per use case","2025-11-29T03:30:00Z","memory_research","codex-agent"]
["d","chunks","Use 256-384 token chunks with 20-30% overlap","320-64","Consensus across SynapticRAG, TempRALM, BEAM research","2025-11-29T03:45:00Z","memory_research","codex-agent"]
["f","compression","Summarization achieves 3-10x safe compression, 15-30x lossy","","From Letta/MemGPT research","2025-11-29T04:00:00Z","memory_research","codex-agent"]
["f","format","Fixed-order arrays save 30-40% tokens vs keyed JSON","","Tested: [\"d\",\"topic\",\"text\"] vs {\"t\":\"d\",\"topic\":\"...\"}","2025-11-29T04:15:00Z","memory_research","codex-agent"]
["q","embedding","Which embedding model for local use?","","Candidates: e5-large-v2, nomic-embed-text-v1.5","2025-11-29T04:30:00Z","memory_planning","open"]
["a","research","Launch 4 Codex agents for memory system research","completed","Temporal, Anchor, Production, Compression tracks","2025-11-29T05:00:00Z","memory_research","claude-code"]
["n","insight","Letta uses 3 tiers: core (in-context), recall (vector), archive (compressed)","","Maps well to our anchors + chunks + summaries structure","2025-11-29T05:15:00Z","memory_research","synthesis"]
["d","temporal","Encode time via sinusoidal embeddings + exponential decay at retrieval","sinusoidal+decay","Separates time from text embeddings, allows tunable decay parameter","","research_session","log-extract"]
["d","hierarchy","Use 3-level hierarchy: 256-384 token leaves, 1k session summaries, 2k long-term summaries","3-level","Balances granularity with compression, matches BEAM/LIGHT patterns","","research_session","log-extract"]
["d","chunking","320 token chunks with 64 token overlap","320/64","Standard across TempRALM and recommendations, prevents context loss","","research_session","log-extract"]
["d","embeddings","Use E5-large-v2 or nomic-embed-text locally, text-embedding-3-large for API","E5/nomic/OpenAI","High-quality sentence embeddings with good OSS support","","research_session","log-extract"]
["d","storage","SQLite schema with id, parent_id, bucket, timestamp, embedding, text columns","SQLite","Simple, bash-friendly, supports hierarchy and temporal metadata","","research_session","log-extract"]
["d","retrieval","Combine vector similarity with time decay: score = sim + β·exp(-Δt/τ)","hybrid-scoring","Balances semantic relevance with recency, tunable via β and τ","","research_session","log-extract"]
["a","compression","Implement SeCom-style KMeans clustering for old chunks","","Reduces memory footprint for aged data","","research_session","log-extract"]
["f","decay-params","Typical decay tau = 7 days, beta = 0.3-0.4","","Standard parameters from TempRALM research","","research_session","log-extract"]
["f","agent-roles","Four roles: Ingestor, Summarizer, Retriever, Compressor","","Division of labor for bash pipeline","","research_session","log-extract"]
["n","verification","Unit-test decay scoring and dry-run parent/child retrieval","","Critical for validating temporal and hierarchical correctness","","research_session","log-extract"]
["d","embeddings","Use E5-large-v2 for local embeddings","E5-large-v2","Best balance of quality and speed for batch processing","2025-11-29T14:00:00Z","test_session","test"]
["a","memory","Build mem-search helper with filtering support","","","2025-11-29T14:00:00Z","test_session","test"]
["q","architecture","Is deeper hierarchy worth the complexity?","","","2025-11-29T14:00:00Z","test_session","test"]
["f","infrastructure","System runs Ubuntu 22.04 with 32GB RAM and RTX 4090","","","2025-11-29T14:00:00Z","test_session","test"]
["a","retrieval","Implement temporal decay in retrieval scoring with configurable tau","","","2025-11-29T14:00:00Z","test_session","test"]
["a","test","Test action for status update verification","done","Marked done for Test 4","2025-11-29T19:00:00Z","test4","test"]
["a","test","Test action for status update verification","done","Completed for test4","2025-11-29T19:05:00Z","test4","test"]
["d","embeddings","Use E5-large-v2 locally for embeddings","E5-large-v2","Local deployment choice","","test_session","log-extract"]
["a","memory","Implement mem-search helper","","","","test_session","log-extract"]
["q","architecture","Is deeper hierarchy worth the complexity?","","","","test_session","log-extract"]
["n","test","Integration test entry added by Claude Code","","","2025-11-30T19:00:00Z","integration_test","claude-code"]
["n", null, "Test note", null, null, "2025-11-30T20:19:27Z", null, null]\n["n", "testing", "Test note with metadata", null, null, "2025-11-30T20:19:37Z", null, null]\n["d", "auth", "Use JWT", "JWT tokens", "Industry standard", "2025-11-30T20:19:42Z", null, null]\n["f", null, "Found a bug", null, null, "2025-11-30T20:19:46Z", null, null]\n["a", "testing", "Test action", null, null, "2025-11-30T20:20:13Z", "test_session", "manual_test"]
["d", "architecture", "Use microservices", "microservices", "Better scalability", "2025-11-30T20:20:45Z", "design_review", "meeting_notes"]
["q", "database", "Should we use PostgreSQL or MySQL?", null, null, "2025-11-30T20:21:11Z", "planning", "brainstorm"]
["n", null, "Verification test", null, null, "2025-11-30T20:21:44Z", null, null]
["n", null, "assessment note", null, null, "2025-11-30T20:39:27Z", null, null]
["f", null, "role-based fact", null, null, "2025-11-30T20:51:29Z", null, null]
["n", null, "Fix test", null, null, "2025-11-30T20:55:24Z", null, null]
["n", null, "dup-check", null, null, "2025-11-30T21:55:39Z", null, null]
["d", "embeddings", "Use OpenAI API only for embeddings, no local ML deps", "api-only", "User has no GPU, ChatGPT Pro != API access", "2025-11-30T23:58:20Z", "phase4-5", null]
["d", "retrieval", "Hybrid scoring formula: score = \u03b1\u00b7sim + \u03b2\u00b7exp(-\u0394t/\u03c4)", "\u03b1=1.0, \u03b2=0.3, \u03c4=7", "Balances relevance with recency", "2025-11-30T23:58:25Z", "phase4-5", null]
["d", "workflow", "Claude actively uses memory during conversations - queries context, records decisions", "active-memory", null, "2025-11-30T23:59:23Z", "phase4-5", null]
["a", "git", "Pushed Phase 4-5 to github.com/badnewsgoonies-dot/swarm-memory", null, null, "2025-12-01T00:00:36Z", "phase4-5", null]
["f", "memory", "Cross-chat memory sharing verified working - new chat successfully queried decisions from this session", null, null, "2025-12-01T00:31:11Z", "phase4-5", null]
["f", "memory", "Chats default to reading anchors.jsonl directly instead of using CLI queries - need stronger CLAUDE.md instructions", null, null, "2025-12-01T00:35:47Z", "phase4-5", null]
["f", "memory", "CLAUDE.md instructions successfully guide chats to use CLI queries instead of reading raw JSONL", null, null, "2025-12-01T00:43:32Z", "phase4-5", null]
["d", "embeddings", "Use local embeddings with all-MiniLM-L6-v2, no API key required", "local-model", "Works on CPU, free, 384 dimensions", "2025-12-01T02:05:52Z", "phase4-5", null]
["d", "embeddings", "Local embeddings work without API key using sentence-transformers and all-MiniLM-L6-v2", "local-backend", "Installed on older PC, portable to 1060 PC", "2025-12-01T02:41:31Z", "memory-setup", null]
["f", "setup", "Venv with sentence-transformers needed for embeddings: python3 -m venv .venv && . .venv/bin/activate && pip install sentence-transformers", null, null, "2025-12-01T02:41:31Z", "memory-setup", null]
["f", "sync", "memory.db is gitignored - to sync across machines: git pull, then ./mem-db.sh init && ./mem-db.sh sync", null, null, "2025-12-01T02:41:31Z", "memory-setup", null]
["f", "memory", "CLAUDE.md must explicitly say 'query memory FIRST before exploring code' - chats default to file exploration otherwise", null, null, "2025-12-01T03:42:09Z", "memory-setup", null]
["d", "architecture", "Use knowledge graph (Zep-style) for memory, not just flat anchors", "knowledge-graph", "Better handling of conflicting/evolving facts via bi-temporal tracking and entity relationships", "2025-12-01T04:23:24Z", "planning-session", "claude-opus"]
["d", "architecture", "Use 2-layer sub-agent hierarchy: Head \u2192 Worker \u2192 Archive", "2-layers", "Each hop adds 1-3s latency; 2 layers balances depth vs speed", "2025-12-01T04:23:25Z", "planning-session", "claude-opus"]
["d", "scale", "Target 10K+ anchors for comprehensive knowledge base", "10k-plus", "User wants long-term multi-project memory, not just single session", "2025-12-01T04:23:25Z", "planning-session", "claude-opus"]
["d", "priority", "Prioritize retrieval accuracy over cost/latency", "accuracy-first", "User willing to sacrifice speed for getting the right memories", "2025-12-01T04:23:26Z", "planning-session", "claude-opus"]
["f", "research", "Zep achieves 94.8% accuracy on Deep Memory Retrieval benchmark vs MemGPT 93.4%", null, null, "2025-12-01T04:23:43Z", "planning-session", "claude-opus"]
["f", "research", "H-MEM uses 4-layer hierarchy with index pointers: Domain \u2192 Category \u2192 Trace \u2192 Episode", null, null, "2025-12-01T04:23:43Z", "planning-session", "claude-opus"]
["f", "research", "Mem0 uses extract/update pipeline: LLM extracts candidates, compares to vector DB, chooses add/merge/update/skip", null, null, "2025-12-01T04:23:44Z", "planning-session", "claude-opus"]
["f", "research", "Zep uses bi-temporal modeling: event_time (when fact occurred) + ingestion_time (when recorded) for conflict detection", null, null, "2025-12-01T04:23:44Z", "planning-session", "claude-opus"]
["f", "pitfalls", "Common memory pitfalls: bloat (solved by merge/dedupe), conflicts (bi-temporal tracking), lost-in-middle (hierarchical routing), models ignoring context (explicit formatting)", null, null, "2025-12-01T04:23:45Z", "planning-session", "claude-opus"]
["n", "novelty", "Novel elements: 1) Multi-model CLI orchestration (Claude/Codex/Copilot via bash), 2) Explicit head/sub-agent separation, 3) Query-time decay (more flexible than static), 4) Compact array format (30-40% token savings)", null, null, "2025-12-01T04:23:54Z", "planning-session", "claude-opus"]
["q", "knowledge-graph", "What schema for knowledge graph entities and relationships? Options: Neo4j, SQLite with edges table, or extend chunks table", null, null, "2025-12-01T04:23:55Z", "planning-session", "claude-opus"]
["d", "knowledge-graph", "Use SQLite with separate entities and edges tables for knowledge graph", "sqlite-entities-edges", "Add entities table for extracted concepts, edges table for relationships. Keep chunks as source of truth.", "2025-12-01T04:27:39Z", "planning-session", "user"]
["d", "architecture", "Hierarchical sub-agent memory retrieval: Head \u2192 Mid-tier \u2192 Leaf agents. Each tier cheaper/smaller. Data flows up as compressed summaries, sub-agents only see slices not full history", "hierarchical-retrieval", null, "2025-12-01T04:36:09Z", "early-planning", null]
["d", "memory-format", "Memory stored as compartmentalized blocks, not single summary. Each block: raw text, metadata (type/topic/time/origin), embeddings, AI-facing structure", "structured-blocks", null, "2025-12-01T04:36:10Z", "early-planning", null]
["d", "glyph-language", "AI-native compressed glyph language for memory. DSL format like [M:DESIGN][proj:X][t:timestamp] DEC: key=value WHY: reason. Super structured, short tokens, parseable. Track G: (grounded) vs I: (inferred)", "ai-glyphs", null, "2025-12-01T04:36:11Z", "early-planning", null]
["d", "context-budget", "Reserve ~half context window for memory refresh. Triggers: context usage >60%, novelty detected, user signals importance. Multi-pass refresh through tiers", "context-budgeting", null, "2025-12-01T04:36:41Z", "early-planning", null]
["d", "temporal", "Time as first-class axis in memory. Fields: timestamp, time_bucket (hour/day/week), session_id, seq_in_session. Enables 'last hour/day/week' queries", "time-indexed", null, "2025-12-01T04:36:42Z", "early-planning", null]
["f", "memory-risks", "Memory system risks: 1) Error stacking through multi-step summarization 2) Stale memory bias 3) AI fills gaps (hallucination) 4) Need forgetting/deprecation mechanism 5) User control over what persists", null, null, "2025-12-01T04:37:01Z", "early-planning", null]
["f", "memory-mitigation", "Mitigations: Keep raw refs in summaries (linked_ids), drill-down capability, multi-pass summarization (facts\u2192caveats\u2192combine), mark grounded vs inferred, recency weighting, superseding mechanism", null, null, "2025-12-01T04:37:02Z", "early-planning", null]
["d", "agent-roles", "Agent specialization: Top orchestrator (speaks normal + glyph), Mid-tier (read/write glyph memory, convert raw\u2192blocks), Leaf agents (handle full raw text/code, produce compact glyphs). Sub-agents dont chat, they write glyph-coded updates", "specialized-tiers", null, "2025-12-01T04:37:02Z", "early-planning", null]
["f", "reconstruction", "AI can reconstruct full picture from tiny snippets using world knowledge + priors. Store surgical extractions, expand when needed. Risk: expanded picture may be partly imagined. Mark model_inferred fields separately", null, null, "2025-12-01T04:37:03Z", "early-planning", null]
["d", "stack", "Current stack: Claude orchestrating Codex agents, ranging from 5.1 high thinking down to minis. Hierarchical swarm with job_pool, progress tracking already built", "claude-codex", null, "2025-12-01T04:37:04Z", "early-planning", null]
["d", "memory-schema", "Memory block schema: id, kind (conversation/decision/doc/code/bug/plan), project, created_at, updated_at, summary, body_ref, tags, embedding vector. Bonus: memory-of-memories index for important decisions", "block-schema", null, "2025-12-01T04:37:16Z", "early-planning", null]
["f", "glyph-example", "Glyph format example: [M:PREF][user:U1][t:2025-11-30] P: lang=TS strict=true P: style=verbose SRC: convo:2025-11-30#msg23-37 STATUS: ACTIVE. Add v: for versioning, G: for grounded, I: for inferred", null, null, "2025-12-01T04:37:16Z", "early-planning", null]
["f", "memory-validation", "A/B test proved memory system value: With memory=3 CLI queries, seconds. Without memory=read 5+ files, 1000+ lines, 10x longer for same answer", null, null, "2025-12-01T04:57:37Z", "validation-session", null]
["f", "cross-chat", "Fresh chat with /clear successfully used memory to answer questions it had no prior context for. Proves knowledge transfer works.", null, null, "2025-12-01T04:57:37Z", "validation-session", null]
["d", "claude-md", "CLAUDE.md must explicitly forbid: reading anchors.jsonl directly, launching Explore agents, reading source files for stored knowledge. Chats default to exploration without strong instructions.", "explicit-forbid", null, "2025-12-01T04:57:38Z", "validation-session", null]
["d", "embeddings", "Local embeddings with all-MiniLM-L6-v2 (384 dims) work without API key. sentence-transformers in .venv. Portable - copy memory.db anywhere.", "local-minilm", null, "2025-12-01T04:57:38Z", "validation-session", null]
["f", "workflow", "Recommended workflow: 1) Query memory first 2) Only explore code if memory lacks answer 3) Write key decisions/facts as you go 4) Embed new entries periodically", null, null, "2025-12-01T04:57:38Z", "validation-session", null]
["f", "setup", "Cross-machine sync: git pull gets anchors.jsonl, then ./mem-db.sh init && sync. memory.db is gitignored. Embeddings need venv setup on each machine.", null, null, "2025-12-01T04:57:39Z", "validation-session", null]
["d", "retrieval", "Semantic search uses hybrid scoring: score = \u03b1\u00b7sim + \u03b2\u00b7exp(-\u0394t/\u03c4). Defaults: \u03b1=1.0, \u03b2=0.3, \u03c4=7 days. Balances relevance with recency.", "hybrid-formula", null, "2025-12-01T04:57:54Z", "validation-session", null]
["f", "phases", "Phase 1-2: SQLite foundation. Phase 3: Multi-chat scoping. Phase 4: Embeddings pipeline. Phase 5: Semantic search. All complete and working.", null, null, "2025-12-01T04:57:54Z", "validation-session", null]
["q", "next-steps", "Potential next: auto-embed on write, knowledge graph (entities+edges tables), hooks for auto-inject, GPU acceleration on 1060", null, null, "2025-12-01T04:57:54Z", "validation-session", null]
["f", "testing", "Testing pattern: ask fresh chat a question, compare with vs without memory. Memory-enabled chat found answer in seconds, non-memory chat took 10x longer reading source files.", null, null, "2025-12-01T04:57:55Z", "validation-session", null]
["f", "repo", "GitHub repo: github.com/badnewsgoonies-dot/swarm-memory. Contains all memory system code, CLAUDE.md auto-instructions, anchors.jsonl data.", null, null, "2025-12-01T04:57:55Z", "validation-session", null]
["d", "testing", "Create synthetic massive memory bank (100K+ entries) to validate architecture. Test: 1) Raw recall by ID 2) Fuzzy semantic queries 3) Multi-hop reasoning 4) Superseding/contradictions", "synthetic-testing", null, "2025-12-01T04:59:37Z", "early-planning", null]
["f", "testing-layers", "Three test layers: Layer 1=Raw recall (accuracy, latency). Layer 2=Structured reasoning (time, superseding, multi-hop). Layer 3=Persona/preference consistency", null, null, "2025-12-01T04:59:37Z", "early-planning", null]
["d", "persona", "Store persona as structured memory blocks [M:PREF][M:BACKSTORY][M:VALUES], not prompt spam. Enables: multiple personas, evolution over time, personality shaped by interaction history", "persona-as-memory", null, "2025-12-01T04:59:38Z", "early-planning", null]
["f", "testing-limits", "Synthetic tests validate architecture capability but dont guarantee real-world performance on messy data. Still need real-world eval on noisy chat, half-finished thoughts, inconsistent tags", null, null, "2025-12-01T04:59:38Z", "early-planning", null]
["f", "fuzzy-queries", "Fuzzy query examples: 'What did we decide about X when Y happened?' 'Summarize all times we changed design because of cost.' Requires understanding semantics, pulling multiple memories, compressing coherently", null, null, "2025-12-01T04:59:38Z", "early-planning", null]
["f", "architecture", "Memory OS = skeleton/organs/routing. LLM = neurons doing thinking. Together = custom AI brain. Not reinventing foundation model, but designing brain architecture around it.", null, null, "2025-12-01T05:01:03Z", "early-planning", null]
["f", "architecture", "Three layers: 1) User & Tools 2) Head Orchestrator (big LLM) 3) Sub-agent layer + Memory OS. Head never eats entire memory, only sees compressed tailored chunk.", null, null, "2025-12-01T05:01:04Z", "early-planning", null]
["f", "ingestion", "Ingestion pipeline: Raw chat \u2192 Ingestion Planner (what should become memory?) \u2192 Specialized Writers (decision-writer, pref-writer, summary-writer) \u2192 Memory OS storage with embeddings + metadata", null, null, "2025-12-01T05:01:04Z", "early-planning", null]
["f", "retrieval", "Retrieval path: Head \u2192 Query Planner \u2192 Retriever sub-agents \u2192 Tier-2 summarizers (group/compress) \u2192 Tier-1 summarizer (merge to context bundle) \u2192 back to Head. Data flows down then compresses back up.", null, null, "2025-12-01T05:01:05Z", "early-planning", null]
["f", "research", "Related research: MemGPT (OS-style memory tiers), H-MEM (hierarchical with index pointers), HiAgent (working memory management), LiCoMemory (multi-level surveys). Our design aligns with current research wave.", null, null, "2025-12-01T05:01:05Z", "early-planning", null]
["f", "frameworks", "Related frameworks: GAM (general agentic memory), AgentVectorDB (LanceDB patterns), MemoryGPT variants, LangMem/LangGraph memory patterns, HyDRA, EvoAgentX", null, null, "2025-12-01T05:01:05Z", "early-planning", null]
["d", "novelty", "Our unique twists: 1) Deep cost-tiered sub-agent chain 2) Explicit AI-glyph language for internal memory 3) Hard context budget rule for memory refresh 4) Synthetic eval baked into design", "four-differentiators", null, "2025-12-01T05:01:06Z", "early-planning", null]
["d", "memgpt-mapping", "MemGPT mapping: CORE=head context+pinned bundle, RECALL=glyphs+vector indices, ARCHIVE=raw logs/docs. Paging policy: when context>X% trigger memory refresh agent to summarize\u2192RECALL, retire\u2192ARCHIVE", "three-tiers", null, "2025-12-01T05:02:13Z", "early-planning", null]
["d", "hmem-mapping", "H-MEM mapping: Add lvl and CHILDREN/parent fields to glyphs. Maintain L1/L2 indices. Coarse-to-fine: retrieve lvl:1 by similarity, follow pointers to lvl:2 and raw, summarize bottom-up", "pointer-hierarchy", null, "2025-12-01T05:02:13Z", "early-planning", null]
["d", "memory-types", "Memory type field: semantic (stable facts/prefs), episodic (session summaries, time-indexed), preference, plan, bug, code. Query planner routes by type+time.", "typed-memory", null, "2025-12-01T05:02:13Z", "early-planning", null]
["f", "glyph-schema", "Extended glyph schema: [M:TYPE][lvl:1][id:X][proj:Y] with fields: CHILDREN (pointer to children), RAW_REFS (links to archive), parent, TIME_BUCKET (day/week). Enables H-MEM style coarse-to-fine", null, null, "2025-12-01T05:02:14Z", "early-planning", null]
["f", "importance", "Importance scoring on write: importance={low,medium,high}. High stays in RECALL, low gets summarized more aggressively. From MemGPT pattern.", null, null, "2025-12-01T05:02:14Z", "early-planning", null]
["n", "session-context", "Session vibe: Built Phase 4-5 (embeddings+semantic), validated with A/B testing, proved fresh chats can use memory. User pasting early planning transcripts to fill memory bank. Rapid iteration, pragmatic approach.", null, null, "2025-12-01T05:04:09Z", "current-session", null]
["n", "handoff", "Next chat should: 1) Query memory first 2) Continue adding content from early planning transcripts 3) Embed new entries periodically 4) Keep pragmatic - build what works, test it", null, null, "2025-12-01T05:04:09Z", "current-session", null]
["f", "current-state", "Memory at 107+ chunks, 100% embedded after each batch. Local embeddings working (all-MiniLM-L6-v2). Cross-chat validated. Early planning content being imported from prior ChatGPT conversations.", null, null, "2025-12-01T05:04:09Z", "current-session", null]
["f", "memgpt", "MemGPT 4-tier architecture", null, null, "2025-12-01T05:09:49Z", null, null]
["f", "memgpt", "MemGPT key pattern: LLM as memory manager", null, null, "2025-12-01T05:09:49Z", null, null]
["f", "memgpt", "MemGPT compression strategy", null, null, "2025-12-01T05:09:50Z", null, null]
["f", "memgpt", "MemGPT failure modes", null, null, "2025-12-01T05:09:52Z", null, null]
["f", "letta", "Letta = MemGPT operationalized as framework", null, null, "2025-12-01T05:10:42Z", null, null]
["f", "letta", "Letta core memory design", null, null, "2025-12-01T05:10:42Z", null, null]
["f", "letta", "Letta shared memory pattern", null, null, "2025-12-01T05:10:43Z", null, null]
["f", "letta", "Letta pitfalls", null, null, "2025-12-01T05:10:45Z", null, null]
["f", "zep", "Zep = temporal knowledge graph memory service", null, null, "2025-12-01T05:11:41Z", null, null]
["f", "zep", "Zep bi-temporal model", null, null, "2025-12-01T05:11:42Z", null, null]
["f", "zep", "Zep layered retrieval", null, null, "2025-12-01T05:11:44Z", null, null]
["f", "zep", "Zep benchmark results", null, null, "2025-12-01T05:11:45Z", null, null]
["f", "zep", "Zep tradeoffs", null, null, "2025-12-01T05:11:47Z", null, null]
["f", "mem0", "Mem0 = two-phase pipeline (Extraction + Update)", null, null, "2025-12-01T05:12:58Z", null, null]
["f", "mem0", "Mem0 memory state per session", null, null, "2025-12-01T05:12:59Z", null, null]
["f", "mem0", "Mem0 ADD/UPDATE/DELETE/NOOP consolidation", null, null, "2025-12-01T05:13:01Z", null, null]
["f", "mem0", "Mem0 benchmark results", null, null, "2025-12-01T05:13:02Z", null, null]
["f", "mem0", "Mem0 tradeoffs", null, null, "2025-12-01T05:13:03Z", null, null]
["f", "h-mem", "H-MEM = 4-layer hierarchy research prototype", null, null, "2025-12-01T05:13:58Z", null, null]
["f", "h-mem", "H-MEM coarse-to-fine retrieval", null, null, "2025-12-01T05:13:59Z", null, null]
["f", "h-mem", "H-MEM memory weights", null, null, "2025-12-01T05:14:00Z", null, null]
["f", "h-mem", "H-MEM challenges", null, null, "2025-12-01T05:14:01Z", null, null]
["f", "research", "TOON format: compact notation beats JSON", null, null, "2025-12-01T05:15:28Z", null, null]
["f", "research", "Recall vs archive heuristics across systems", null, null, "2025-12-01T05:15:29Z", null, null]
["f", "research", "Benchmark evidence: memory systems beat long context", null, null, "2025-12-01T05:15:30Z", null, null]
["f", "pitfalls", "Common memory system pitfalls", null, null, "2025-12-01T05:15:32Z", null, null]
["f", "research", "Multi-level indices in production", null, null, "2025-12-01T05:15:33Z", null, null]
["f", "frameworks", "Framework patterns: LangGraph, AutoGen, Swarm", null, null, "2025-12-01T05:15:35Z", null, null]
["f", "comparison", "Design alignment with existing systems", null, null, "2025-12-01T05:17:02Z", null, null]
["d", "recommendations", "5 design recommendations from research", "All 5", "1) Explicit Core/Recall/Archive tiers 2) Anchor glyphs with type/topic/time metadata 3) Hierarchical coarse-to-fine retrieval 4) ADD/UPDATE/DELETE/NOOP consolidation 5) Multi-agent but keep chain to 2-3 hops max", "2025-12-01T05:17:03Z", null, null]
["d", "recommendations", "3 risks to mitigate", "Watch carefully", "1) Pipeline complexity/latency - monitor hops per query 2) Glyph language needs prompt examples or LLM ignores it 3) Over-summarization loses info - never summarize a summary, keep raw for fresh summaries", "2025-12-01T05:17:05Z", null, null]
["f", "novelty", "3 novel aspects to focus on", null, null, "2025-12-01T05:17:06Z", null, null]
["f", "implementation", "Anchor glyph format recommendation", null, null, "2025-12-01T05:17:08Z", null, null]
["f", "architecture", "Multi-chat memory sharing is trivial with external memory", null, null, "2025-12-01T05:19:24Z", null, null]
["f", "architecture", "Working buffer: shared vs private glyphs", null, null, "2025-12-01T05:19:24Z", null, null]
["f", "architecture", "Multi-chat role specialization", null, null, "2025-12-01T05:19:25Z", null, null]
["f", "architecture", "Transaction safety for multi-chat", null, null, "2025-12-01T05:19:27Z", null, null]
["f", "autonomy", "LLMs cannot act unprompted - fundamental design constraint", null, null, "2025-12-01T05:41:06Z", null, null]
["f", "autonomy", "Simulating autonomous behavior via wrappers", null, null, "2025-12-01T05:41:07Z", null, null]
["f", "autonomy", "Autonomous agent architecture pattern", null, null, "2025-12-01T05:41:59Z", null, null]
["f", "autonomy", "Tool protocol for autonomous agents", null, null, "2025-12-01T05:42:00Z", null, null]
["f", "autonomy", "Event-driven vs polling for autonomous agents", null, null, "2025-12-01T05:42:01Z", null, null]
["f", "autonomy", "Autonomous agent safety checklist", null, null, "2025-12-01T05:42:02Z", null, null]
["f", "architecture", "Repo components mapped to autonomous agent architecture", null, null, "2025-12-01T05:43:31Z", null, null]
["f", "implementation", "swarm_daemon.py proposal", null, null, "2025-12-01T05:43:31Z", null, null]
["f", "implementation", "JSON action protocol for autonomous head", null, null, "2025-12-01T05:43:32Z", null, null]
["q", "implementation", "Next upgrades for autonomous daemon", null, null, "2025-12-01T05:43:34Z", null, null]
["f", "autonomy", "Route to autonomous-feeling LLM system", null, null, "2025-12-01T05:44:42Z", null, null]
["f", "autonomy", "5-step evolution toward autonomous agent", null, null, "2025-12-01T05:44:44Z", null, null]
["f", "autonomy", "Event-driven agent loop pattern", null, null, "2025-12-01T05:44:45Z", null, null]
["f", "autonomy", "Bounded self-modification pattern", null, null, "2025-12-01T05:44:47Z", null, null]
["f", "monetization", "4 monetization patterns for agent/memory systems", null, null, "2025-12-01T05:47:57Z", null, null]
["f", "market", "Real-world agent examples in production", null, null, "2025-12-01T05:47:57Z", null, null]
["f", "market", "Real-world limitations of agent systems", null, null, "2025-12-01T05:47:58Z", null, null]
["f", "market", "Market positioning for memory OS", null, null, "2025-12-01T05:48:00Z", null, null]
["f", "market", "10 structural blockers to sophisticated AI agents", null, null, "2025-12-01T05:48:52Z", null, null]
["f", "market", "Why this project matters: filling the plumbing gap", null, null, "2025-12-01T05:48:53Z", null, null]
["f", "market", "Agent failure modes", null, null, "2025-12-01T05:48:54Z", null, null]
["n", "templates", "Research prompt template for deep dives", null, null, "2025-12-01T05:51:13Z", null, null]
