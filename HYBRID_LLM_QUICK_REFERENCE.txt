================================================================================
HYBRID LLM ARCHITECTURE - QUICK REFERENCE CARD
================================================================================

ARCHITECTURE OVERVIEW
─────────────────────
3-Tier System: LOCAL_FAST → LOCAL_QUALITY → API_FALLBACK
Cost Savings: 75% vs pure API
Quality: Automatic fallback ensures >95% pass rate

TIERS
─────
Tier 1: LOCAL_FAST (Phi-3, Llama 8B)
  Use: Classification, tagging, yes/no
  Speed: 50-200ms | Cost: $0

Tier 2: LOCAL_QUALITY (Mixtral, Codestral)
  Use: Code generation, reasoning
  Speed: 200-2000ms | Cost: $0

Tier 3: API_FALLBACK (Claude, GPT-4)
  Use: Orchestration, complex tasks
  Speed: 1-5s | Cost: $2.5-3/M tokens

SETUP COMMANDS
──────────────
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull models (one-time)
ollama pull phi3:mini        # 3.8GB
ollama pull llama3:8b        # 4.7GB
ollama pull mixtral:8x7b     # 26GB
ollama pull codestral:22b    # 12GB

# Set API keys
export ANTHROPIC_API_KEY="sk-ant-..."
export OPENAI_API_KEY="sk-..."

# Test installation
ollama run phi3:mini "test"

TESTING COMMANDS
────────────────
# Test router
python llm_router.py \
  --prompt "Extract topic from: Fixed auth bug" \
  --action classify_memory \
  --quality-check

# Run cost simulation
python cost_analysis.py --simulate --days 30

# Test integration
python swarm_daemon_hybrid_example.py \
  --objective "Test task" \
  --verbose

# Check status
python swarm_daemon_hybrid_example.py --status

FILES
─────
Core:
  llm_router.py                  - Router implementation (637 lines)
  llm_config.yaml                - Configuration (195 lines)
  cost_analysis.py               - Cost simulator (414 lines)
  swarm_daemon_hybrid_example.py - Integration example (290 lines)

Docs:
  HYBRID_LLM_INDEX.md            - Start here (navigation)
  README_HYBRID_LLM.md           - Quick start guide
  ARCHITECTURE_HYBRID_LLM.md     - Technical design
  HYBRID_LLM_VISUAL_SUMMARY.md   - Diagrams and charts
  HYBRID_LLM_DELIVERABLES.md     - Summary and metrics

CONFIGURATION
─────────────
Edit llm_config.yaml:

# Adjust quality threshold
routing:
  quality_threshold: 0.7  # 0.0-1.0 (higher = stricter)

# Set cost limits
cost_control:
  daily_budget: 10.0      # USD

# Override action routing
routing:
  action_tier_overrides:
    spawn_daemon: "api_fallback"

INTEGRATION
───────────
Minimal (5 min):
  1. Copy llm_router.py, llm_config.yaml
  2. Install Ollama, pull phi3:mini
  3. Add to swarm_daemon.py:
       from llm_router import LLMRouter
       router = LLMRouter()
  4. Replace call_llm() with router calls
  5. Test with --max-iterations 5

Full (30 min):
  See swarm_daemon_hybrid_example.py for complete example

MONITORING
──────────
# Query costs
./mem-db.sh query topic=llm_cost recent=24h

# Export usage
python -c "from llm_router import LLMRouter; \
           router = LLMRouter(); \
           router.export_usage_log('usage.jsonl')"

# View stats
cat usage.jsonl | jq '.'

COST ESTIMATES (100 iterations/day)
───────────────────────────────────
Light (50/day):   Pure API $108/mo → Hybrid $14/mo  (87% savings)
Medium (200/day): Pure API $474/mo → Hybrid $66/mo  (86% savings)
Heavy (500/day):  Pure API $1327/mo → Hybrid $337/mo (75% savings)

TROUBLESHOOTING
───────────────
Ollama not found:
  curl -fsSL https://ollama.com/install.sh | sh
  ollama serve

Model not found:
  ollama pull phi3:mini
  ollama list

Quality checks fail:
  Edit llm_config.yaml:
    routing:
      quality_threshold: 0.6  # Lower from 0.7

High API fallback:
  Enable more local models in llm_config.yaml

QUALITY CHECKS
──────────────
Confidence = Format(0.3) + Complete(0.2) + Coherent(0.3) + Critique(0.2)
Pass if: confidence >= 0.7
Fail: Automatic fallback to next tier

DECISION TREE
─────────────
Request → Classify complexity
   ├─ SIMPLE → Tier 1 (LOCAL_FAST)
   ├─ MODERATE → Tier 2 (LOCAL_QUALITY)
   └─ COMPLEX → Tier 3 (API_FALLBACK)

SUCCESS METRICS (Week 1)
────────────────────────
✓ Cost < $5/week
✓ Savings > 70%
✓ Latency < 500ms avg
✓ Quality pass > 95%
✓ Local calls > 60%

NEXT STEPS
──────────
1. Read README_HYBRID_LLM.md
2. Install Ollama and models
3. Test router with examples
4. Run cost simulation
5. Integrate with daemon
6. Monitor for 1 week
7. Optimize configuration

SUPPORT
───────
Documentation: See HYBRID_LLM_INDEX.md for full file list
Logs: daemon_hybrid.log
Config: llm_config.yaml
Issues: Check troubleshooting section above

================================================================================
Last Updated: 2025-12-01 | Version: 1.0 | Status: Production Ready
================================================================================
